# -*- coding: utf-8 -*-
"""aishwariyaproj.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZJs7OXCSIS23l9VkRURFJ3ZM3TqWohNQ
"""

# ============================================================================
# COMPLETE MALWARE DETECTION SYSTEM
# ============================================================================

import gradio as gr
import pandas as pd
import numpy as np
import PyPDF2
import docx
import json
import re
import os
import io
import time
import joblib
import warnings
from collections import Counter
import math

# ML Libraries
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, confusion_matrix, classification_report,
    roc_auc_score, roc_curve
)
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import shap

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

warnings.filterwarnings('ignore')

print("‚úÖ All imports successful!\n")

# ============================================================================
# SECTION 2: DATASET DOWNLOAD AND LOADING
# ============================================================================

print("üì• Downloading CIC-Evasive-PDFMal2022 Dataset...")
print("Note: You need to upload the dataset manually or download from Kaggle")

# Load the dataset from local file
dataset_file = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'PDFMalware2022.parquet')
print(f"\nüîπ Loading dataset from: {dataset_file}")
df = pd.read_parquet(dataset_file)

print(f"\n‚úÖ Dataset loaded successfully!")
print(f"   Shape: {df.shape}")
print(f"   Columns: {df.columns.tolist()[:5]}...")
print(f"\nüìä Dataset Preview:")
print(df.head())

# ============================================================================
# SECTION 3: DATA PREPROCESSING
# ============================================================================

print("\n" + "="*80)
print("üîß DATA PREPROCESSING")
print("="*80)

# Check for missing values
print("\nüìã Missing Values:")
print(df.isnull().sum().sum())

# Check class distribution
print("\nüìä Class Distribution:")
print(df['Class'].value_counts())

# Separate features and target
X = df.drop('Class', axis=1)
y = df['Class']

# Drop 'FileName' column as it's an identifier and not a feature
if 'FileName' in X.columns:
    X = X.drop('FileName', axis=1)

# Convert all 'object' type columns that are essentially binary (like 'Yes'/'No') to numeric
for col in X.columns:
    if X[col].dtype == 'object':
        unique_values = X[col].dropna().unique()
        if set(unique_values).issubset({'Yes', 'No'}):
            X[col] = X[col].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)
        elif set(unique_values).issubset({'True', 'False'}):
            X[col] = X[col].map({'True': 1, 'False': 0}).fillna(0).astype(int)
        else:
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except:
                print(f"Warning: Column '{col}' is object type. Applying LabelEncoder.")
                le_feature = LabelEncoder()
                X[col] = X[col].fillna('__MISSING__')
                X[col] = le_feature.fit_transform(X[col])
    elif pd.api.types.is_categorical_dtype(X[col]):
        X[col] = X[col].cat.codes
        X[col] = X[col].replace(-1, 0)

# Encode target variable
if y.dtype == 'object' or pd.api.types.is_categorical_dtype(y):
    le = LabelEncoder()
    y = le.fit_transform(y)
    print(f"\nüè∑Ô∏è  Classes: {le.classes_}")
else:
    print(f"\nüè∑Ô∏è  Classes: 0=Benign, 1=Malicious")

# Handle any remaining missing values
X = X.fillna(0)

# Get feature names
feature_names = X.columns.tolist()
print(f"\n‚úÖ Total Features: {len(feature_names)}")
print(f"   Features: {feature_names[:10]}...")

# ============================================================================
# CRITICAL: INSPECT ACTUAL DATASET FEATURES
# ============================================================================

print("\n" + "="*80)
print("üîç INSPECTING DATASET FEATURES FOR PROPER MAPPING")
print("="*80)

print(f"\nüìã All {len(feature_names)} features in the dataset:")
for i, fname in enumerate(feature_names, 1):
    print(f"   {i:2d}. {fname}")

# Get feature statistics to understand the data
print(f"\nüìä Feature Statistics (sample):")
for fname in feature_names[:5]:
    non_zero_count = (X[fname] != 0).sum()
    print(f"\n   {fname}:")
    print(f"      Range: [{X[fname].min():.2f}, {X[fname].max():.2f}]")
    print(f"      Mean: {X[fname].mean():.2f}")
    print(f"      Non-zero values: {non_zero_count} / {len(X)} ({non_zero_count/len(X)*100:.1f}%)")

print("="*80 + "\n")

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print(f"\nüìä Data Split:")
print(f"   Training samples: {X_train.shape[0]}")
print(f"   Testing samples: {X_test.shape[0]}")

# Handle class imbalance with SMOTE
print("\n‚öñÔ∏è  Applying SMOTE for class balancing...")
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"   After SMOTE - Training samples: {X_train_balanced.shape[0]}")
print(f"   Class distribution: {np.bincount(y_train_balanced)}")

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_balanced)
X_test_scaled = scaler.transform(X_test)

print("\n‚úÖ Preprocessing complete!")

# ============================================================================
# SECTION 4: MODEL TRAINING
# ============================================================================

print("\n" + "="*80)
print("ü§ñ MODEL TRAINING")
print("="*80)

models = {
    'Random Forest': RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        random_state=42,
        n_jobs=-1,
        class_weight='balanced'
    ),
    'XGBoost': XGBClassifier(
        n_estimators=200,
        max_depth=10,
        learning_rate=0.1,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
        eval_metric='logloss',
        use_label_encoder=False
    ),
    'Neural Network': MLPClassifier(
        hidden_layer_sizes=(128, 64, 32),
        activation='relu',
        solver='adam',
        alpha=0.001,
        batch_size=128,
        learning_rate='adaptive',
        max_iter=500,
        random_state=42,
        early_stopping=True
    )
}

trained_models = {}
model_scores = {}

for name, model in models.items():
    print(f"\nüîÑ Training {name}...")
    start_time = time.time()

    model.fit(X_train_scaled, y_train_balanced)
    trained_models[name] = model

    y_pred = model.predict(X_test_scaled)
    accuracy = accuracy_score(y_test, y_pred)
    model_scores[name] = accuracy

    training_time = time.time() - start_time
    print(f"   ‚úÖ {name} trained in {training_time:.2f}s")
    print(f"   üìä Test Accuracy: {accuracy:.4f}")

print("\n‚úÖ Ensemble model ready!")

# ============================================================================
# SECTION 5: MODEL EVALUATION
# ============================================================================

print("\n" + "="*80)
print("üìà MODEL EVALUATION")
print("="*80)

rf_proba = trained_models['Random Forest'].predict_proba(X_test_scaled)
xgb_proba = trained_models['XGBoost'].predict_proba(X_test_scaled)
nn_proba = trained_models['Neural Network'].predict_proba(X_test_scaled)

ensemble_proba = (0.4 * rf_proba + 0.4 * xgb_proba + 0.2 * nn_proba)
ensemble_pred = np.argmax(ensemble_proba, axis=1)

accuracy = accuracy_score(y_test, ensemble_pred)
precision = precision_score(y_test, ensemble_pred)
recall = recall_score(y_test, ensemble_pred)
f1 = f1_score(y_test, ensemble_pred)
roc_auc = roc_auc_score(y_test, ensemble_proba[:, 1])

print(f"\nüéØ ENSEMBLE MODEL PERFORMANCE:")
print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"   Precision: {precision:.4f}")
print(f"   Recall:    {recall:.4f}")
print(f"   F1-Score:  {f1:.4f}")
print(f"   ROC-AUC:   {roc_auc:.4f}")

cm = confusion_matrix(y_test, ensemble_pred)
print(f"\nüìä Confusion Matrix:")
print(f"   True Negatives:  {cm[0,0]}")
print(f"   False Positives: {cm[0,1]}")
print(f"   False Negatives: {cm[1,0]}")
print(f"   True Positives:  {cm[1,1]}")

fpr = cm[0,1] / (cm[0,0] + cm[0,1])
print(f"\n   False Positive Rate: {fpr:.4f} ({fpr*100:.2f}%)")

print(f"\nüìã Detailed Classification Report:")
print(classification_report(y_test, ensemble_pred, target_names=['Benign', 'Malicious']))

feature_importance = trained_models['Random Forest'].feature_importances_
top_features_idx = np.argsort(feature_importance)[-10:][::-1]

print(f"\nüîù Top 10 Important Features:")
for i, idx in enumerate(top_features_idx, 1):
    print(f"   {i}. {feature_names[idx]}: {feature_importance[idx]:.4f}")

# Visualizations
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
axes[0,0].set_title('Confusion Matrix', fontsize=14, fontweight='bold')
axes[0,0].set_xlabel('Predicted')
axes[0,0].set_ylabel('Actual')

model_names = list(model_scores.keys()) + ['Ensemble']
model_accuracies = list(model_scores.values()) + [accuracy]
axes[0,1].bar(model_names, model_accuracies, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
axes[0,1].set_title('Model Accuracy Comparison', fontsize=14, fontweight='bold')
axes[0,1].set_ylabel('Accuracy')
axes[0,1].set_ylim([0.9, 1.0])
for i, v in enumerate(model_accuracies):
    axes[0,1].text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')

top_10_features = [feature_names[i] for i in top_features_idx]
top_10_importance = [feature_importance[i] for i in top_features_idx]
axes[1,0].barh(top_10_features, top_10_importance, color='teal')
axes[1,0].set_title('Top 10 Feature Importance', fontsize=14, fontweight='bold')
axes[1,0].set_xlabel('Importance')
axes[1,0].invert_yaxis()

fpr_roc, tpr_roc, _ = roc_curve(y_test, ensemble_proba[:, 1])
axes[1,1].plot(fpr_roc, tpr_roc, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')
axes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')
axes[1,1].set_xlim([0.0, 1.0])
axes[1,1].set_ylim([0.0, 1.05])
axes[1,1].set_xlabel('False Positive Rate')
axes[1,1].set_ylabel('True Positive Rate')
axes[1,1].set_title('ROC Curve', fontsize=14, fontweight='bold')
axes[1,1].legend(loc="lower right")
axes[1,1].grid(alpha=0.3)

plt.tight_layout()
plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
plt.show()

print("\n‚úÖ Visualizations saved as 'model_evaluation.png'")

# ============================================================================
# SECTION 6: IMPROVED FEATURE EXTRACTION (FIXED VERSION)
# ============================================================================

print("\n" + "="*80)
print("üîß IMPROVED FEATURE EXTRACTION FUNCTIONS")
print("="*80)

class ImprovedFeatureExtractor:
    """Extract features matching CIC-Evasive-PDFMal2022 dataset structure"""

    def __init__(self, feature_names, feature_stats):
        self.feature_names = feature_names
        self.num_features = len(feature_names)
        self.feature_stats = feature_stats  # Store mean/std for each feature
        print(f"üìã Initialized with {self.num_features} features")
        print(f"üìã Sample features: {feature_names[:5]}")

    def extract_pdf_features(self, file_content):
        """Extract features from PDF file with exact dataset mapping"""
        # Initialize all features to their mean values from training data
        features = {fname: self.feature_stats.get(fname, {}).get('mean', 0)
                   for fname in self.feature_names}

        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
            pdf_content = file_content.decode('latin-1', errors='ignore')
            metadata = pdf_reader.metadata if pdf_reader.metadata else {}

            # Extract all possible PDF features
            file_size_kb = len(file_content) / 1024
            num_pages = len(pdf_reader.pages)

            # Create comprehensive feature mapping
            extracted_features = {
                # Size features
                'pdfsize': len(file_content),
                'PdfSize': file_size_kb,
                'size': len(file_content),
                'filesize': file_size_kb,

                # Page features
                'pages': num_pages,
                'Pages': num_pages,
                'PageNo': num_pages,
                'page': pdf_content.count('/Page'),

                # Metadata
                'metadata_size': len(str(metadata)),
                'MetadataSize': len(str(metadata)),
                'title_characters': len(str(metadata.get('/Title', ''))),
                'TitleCharacters': len(str(metadata.get('/Title', ''))),
                'author_characters': len(str(metadata.get('/Author', ''))),
                'AuthorCharacters': len(str(metadata.get('/Author', ''))),
                'producer_characters': len(str(metadata.get('/Producer', ''))),
                'ProducerCharacters': len(str(metadata.get('/Producer', ''))),
                'creator_characters': len(str(metadata.get('/Creator', ''))),
                'CreatorCharacters': len(str(metadata.get('/Creator', ''))),

                # Structure counts
                'xref_length': pdf_content.count('xref'),
                'XrefLength': pdf_content.count('xref'),
                'obj': pdf_content.count('obj'),
                'Obj': pdf_content.count('obj'),
                'endobj': pdf_content.count('endobj'),
                'Endobj': pdf_content.count('endobj'),
                'stream': pdf_content.count('stream'),
                'Stream': pdf_content.count('stream'),
                'endstream': pdf_content.count('endstream'),
                'Endstream': pdf_content.count('endstream'),
                'xref': pdf_content.count('xref'),
                'Xref': pdf_content.count('xref'),
                'trailer': pdf_content.count('trailer'),
                'Trailer': pdf_content.count('trailer'),
                'startxref': pdf_content.count('startxref'),
                'StartXref': pdf_content.count('startxref'),

                # Security/Encryption
                'isEncrypted': 1 if pdf_reader.is_encrypted else 0,
                'encrypted': 1 if pdf_reader.is_encrypted else 0,
                'Encrypt': pdf_content.count('/Encrypt'),
                'encrypt': pdf_content.count('/Encrypt'),
                'flags': 1 if '/Encrypt' in pdf_content else 0,

                # Suspicious elements
                'Javascript': 1 if ('/JavaScript' in pdf_content or '/JS' in pdf_content) else 0,
                'javascript': pdf_content.count('/JavaScript'),
                'JS': pdf_content.count('/JS'),
                'js': pdf_content.count('/JS'),

                # Actions
                'AutoAction': 1 if '/OpenAction' in pdf_content else 0,
                'OpenAction': pdf_content.count('/OpenAction'),
                'openaction': pdf_content.count('/OpenAction'),
                'AA': pdf_content.count('/AA'),
                'aa': pdf_content.count('/AA'),
                'Launch': pdf_content.count('/Launch'),
                'launch': pdf_content.count('/Launch'),

                # Forms
                'AcroForm': 1 if '/AcroForm' in pdf_content else 0,
                'Acroform': pdf_content.count('/AcroForm'),
                'acroform': pdf_content.count('/AcroForm'),

                # Embedded content
                'EmbeddedFile': 1 if '/EmbeddedFile' in pdf_content else 0,
                'EmbeddedFiles': pdf_content.count('/EmbeddedFile'),
                'embeddedfile': pdf_content.count('/EmbeddedFile'),

                # Other features
                'ObjStm': pdf_content.count('/ObjStm'),
                'objstm': pdf_content.count('/ObjStm'),
                'JBIG2Decode': pdf_content.count('/JBIG2Decode'),
                'jbig2decode': pdf_content.count('/JBIG2Decode'),
                'RichMedia': pdf_content.count('/RichMedia'),
                'richmedia': pdf_content.count('/RichMedia'),
                'XFA': 1 if '/XFA' in pdf_content else 0,
                'xfa': pdf_content.count('/XFA'),
                'Colors': 0,
                'colors_gt_2_pow_24': 0,

                # Content flags
                'Images': pdf_content.count('/Image'),
                'images': pdf_content.count('/Image'),
                'Text': 1 if len(pdf_content) > 0 else 0,
                'text': 1 if len(pdf_content) > 0 else 0,
                'Header': 1 if pdf_content.startswith('%PDF') else 0,
                'header': 1 if pdf_content.startswith('%PDF') else 0,
            }

            # Update features with extracted values (case-sensitive matching)
            for key, value in extracted_features.items():
                if key in features:
                    features[key] = value

            # Debug output
            non_zero = {k: v for k, v in features.items() if v != 0 and k in self.feature_names[:10]}
            if non_zero:
                print(f"‚úÖ PDF: Extracted {len([v for v in features.values() if v != 0])} non-zero features")
                print(f"   Sample: {list(non_zero.items())[:3]}")

        except Exception as e:
            print(f"‚ö†Ô∏è  PDF extraction error: {str(e)}")
            import traceback
            traceback.print_exc()

        # Create feature vector in exact order
        feature_vector = np.array([features[fname] for fname in self.feature_names]).reshape(1, -1)
        return feature_vector, features

    def extract_docx_features(self, file_content):
        """Extract features from DOCX with proper error handling"""
        features = {fname: self.feature_stats.get(fname, {}).get('mean', 0)
                   for fname in self.feature_names}

        try:
            doc = docx.Document(io.BytesIO(file_content))
            full_text = '\n'.join([para.text for para in doc.paragraphs])

            file_size_kb = len(file_content) / 1024
            num_sections = len(doc.sections)

            extracted_features = {
                'pdfsize': len(file_content),
                'PdfSize': file_size_kb,
                'size': len(file_content),
                'pages': num_sections,
                'Pages': num_sections,
                'metadata_size': len(full_text),
                'MetadataSize': len(full_text),
                'Text': 1 if len(full_text) > 0 else 0,
                'text': 1 if len(full_text) > 0 else 0,
                'Header': 1,
                'header': 1,
                'EmbeddedFile': 1 if ('http' in full_text.lower()) else 0,
                'embeddedfile': 1 if ('http' in full_text.lower()) else 0,
            }

            for key, value in extracted_features.items():
                if key in features:
                    features[key] = value

            print(f"‚úÖ DOCX: Extracted {len([v for v in features.values() if v != 0])} non-zero features")

        except Exception as e:
            print(f"‚ö†Ô∏è  DOCX extraction error: {str(e)}")
            # Use default mean values - already initialized

        feature_vector = np.array([features[fname] for fname in self.feature_names]).reshape(1, -1)
        return feature_vector, features

    def extract_json_features(self, file_content):
        """Extract features from JSON with proper error handling"""
        features = {fname: self.feature_stats.get(fname, {}).get('mean', 0)
                   for fname in self.feature_names}

        try:
            content = file_content.decode('utf-8', errors='ignore')
            file_size_kb = len(file_content) / 1024

            has_scripts = '<script' in content.lower() or 'eval(' in content.lower()
            has_urls = bool(re.search(r'https?://', content))

            extracted_features = {
                'pdfsize': len(file_content),
                'PdfSize': file_size_kb,
                'size': len(file_content),
                'metadata_size': len(content),
                'MetadataSize': len(content),
                'Text': 1 if len(content) > 0 else 0,
                'text': 1 if len(content) > 0 else 0,
                'Javascript': 1 if has_scripts else 0,
                'javascript': 1 if has_scripts else 0,
                'EmbeddedFile': 1 if has_urls else 0,
                'embeddedfile': 1 if has_urls else 0,
            }

            for key, value in extracted_features.items():
                if key in features:
                    features[key] = value

            print(f"‚úÖ JSON: Extracted {len([v for v in features.values() if v != 0])} non-zero features")

        except Exception as e:
            print(f"‚ö†Ô∏è  JSON extraction error: {str(e)}")
            # Use default mean values

        feature_vector = np.array([features[fname] for fname in self.feature_names]).reshape(1, -1)
        return feature_vector, features

# Calculate feature statistics for initialization
feature_stats = {}
for fname in feature_names:
    feature_stats[fname] = {
        'mean': X[fname].mean(),
        'std': X[fname].std(),
        'min': X[fname].min(),
        'max': X[fname].max()
    }

# Initialize improved feature extractor
feature_extractor = ImprovedFeatureExtractor(feature_names, feature_stats)

print("‚úÖ Improved feature extraction ready!")

# ============================================================================
# SECTION 7: PREDICTION FUNCTION WITH DEBUGGING
# ============================================================================

def predict_file_malware(file):
    """Main prediction function for Gradio interface"""

    if file is None:
        return {
            "error": "No file uploaded",
            "prediction": "N/A",
            "confidence": 0,
            "threat_level": "N/A",
            "details": ""
        }

    try:
        with open(file.name, 'rb') as f:
            file_content = f.read()

        filename = os.path.basename(file.name)
        file_ext = os.path.splitext(filename)[1].lower()

        print(f"\n{'='*60}")
        print(f"üîç Analyzing: {filename}")
        print(f"{'='*60}")

        # Extract features based on file type
        if file_ext == '.pdf':
            features_array, features_dict = feature_extractor.extract_pdf_features(file_content)
        elif file_ext in ['.docx', '.doc']:
            features_array, features_dict = feature_extractor.extract_docx_features(file_content)
        elif file_ext == '.json':
            features_array, features_dict = feature_extractor.extract_json_features(file_content)
        else:
            return {
                "error": f"Unsupported file type: {file_ext}",
                "prediction": "N/A",
                "confidence": 0,
                "threat_level": "N/A",
                "details": "Only PDF, DOCX, DOC, and JSON files are supported."
            }

        # Scale features
        features_scaled = scaler.transform(features_array)

        # Get predictions from all models
        rf_proba = trained_models['Random Forest'].predict_proba(features_scaled)[0]
        xgb_proba = trained_models['XGBoost'].predict_proba(features_scaled)[0]
        nn_proba = trained_models['Neural Network'].predict_proba(features_scaled)[0]

        print(f"ü§ñ Individual Model Probabilities:")
        print(f"   RF:  Benign={rf_proba[0]:.4f}, Malicious={rf_proba[1]:.4f}")
        print(f"   XGB: Benign={xgb_proba[0]:.4f}, Malicious={xgb_proba[1]:.4f}")
        print(f"   NN:  Benign={nn_proba[0]:.4f}, Malicious={nn_proba[1]:.4f}")

        # Ensemble prediction (weighted average)
        ensemble_proba = 0.4 * rf_proba + 0.4 * xgb_proba + 0.2 * nn_proba
        prediction_idx = np.argmax(ensemble_proba)
        confidence = ensemble_proba[prediction_idx]

        print(f"üéØ Ensemble: Benign={ensemble_proba[0]:.4f}, Malicious={ensemble_proba[1]:.4f}")
        print(f"üìä Final: {'MALICIOUS' if prediction_idx == 1 else 'BENIGN'} ({confidence*100:.2f}%)")
        print(f"{'='*60}\n")

        # Determine prediction label
        prediction_label = "ü¶† MALICIOUS" if prediction_idx == 1 else "‚úÖ BENIGN"

        # Determine threat level
        if prediction_idx == 0:
            threat_level = "üü¢ SAFE"
        else:
            if confidence >= 0.95:
                threat_level = "üî¥ CRITICAL"
            elif confidence >= 0.85:
                threat_level = "üü† HIGH"
            elif confidence >= 0.70:
                threat_level = "üü° MEDIUM"
            else:
                threat_level = "üü§ LOW"

        # Feature importance
        rf_importance = trained_models['Random Forest'].feature_importances_
        top_features_idx = np.argsort(rf_importance)[-5:][::-1]

        important_features = []
        for idx in top_features_idx:
            feat_name = feature_names[idx]
            feat_value = features_array[0][idx]
            feat_importance = rf_importance[idx]
            important_features.append(f"  ‚Ä¢ {feat_name}: {feat_value:.2f} (importance: {feat_importance:.4f})")

        # Build detailed report
        details = f"""
üìÑ **File Analysis Report**

**File Name:** {filename}
**File Type:** {file_ext.upper()}
**File Size:** {len(file_content) / 1024:.2f} KB

---

**üéØ PREDICTION RESULTS**

**Status:** {prediction_label}
**Confidence:** {confidence * 100:.2f}%
**Threat Level:** {threat_level}

---

**üîç Model Predictions:**

‚Ä¢ Random Forest: {"Malicious" if np.argmax(rf_proba) == 1 else "Benign"} ({rf_proba[np.argmax(rf_proba)] * 100:.2f}%)
‚Ä¢ XGBoost: {"Malicious" if np.argmax(xgb_proba) == 1 else "Benign"} ({xgb_proba[np.argmax(xgb_proba)] * 100:.2f}%)
‚Ä¢ Neural Network: {"Malicious" if np.argmax(nn_proba) == 1 else "Benign"} ({nn_proba[np.argmax(nn_proba)] * 100:.2f}%)

---

**üìä Top 5 Important Features:**

{chr(10).join(important_features)}

---

**‚ö†Ô∏è RECOMMENDATION:**

"""

        if prediction_idx == 1:
            details += """
‚õî **DO NOT OPEN THIS FILE!**

This file has been identified as potentially malicious. It may contain:
‚Ä¢ Embedded malicious scripts
‚Ä¢ Suspicious JavaScript code
‚Ä¢ Hidden executable content
‚Ä¢ Phishing attempts

**Recommended Actions:**
1. Delete the file immediately
2. Run a full system scan
3. Report to your security team
"""
        else:
            details += """
‚úÖ **File appears to be safe**

This file passed all malware detection checks. However:
‚Ä¢ Always exercise caution with files from unknown sources
‚Ä¢ Keep your antivirus software updated
‚Ä¢ Scan files with multiple security tools for maximum protection
"""

        return {
            "prediction": prediction_label,
            "confidence": f"{confidence * 100:.2f}%",
            "threat_level": threat_level,
            "details": details,
            "rf_confidence": f"{rf_proba[np.argmax(rf_proba)] * 100:.1f}%",
            "xgb_confidence": f"{xgb_proba[np.argmax(xgb_proba)] * 100:.1f}%",
            "nn_confidence": f"{nn_proba[np.argmax(nn_proba)] * 100:.1f}%"
        }

    except Exception as e:
        print(f"‚ùå ERROR: {str(e)}")
        import traceback
        traceback.print_exc()
        return {
            "error": str(e),
            "prediction": "ERROR",
            "confidence": "0%",
            "threat_level": "N/A",
            "details": f"An error occurred during analysis: {str(e)}"
        }

# ============================================================================
# SECTION 8: GRADIO INTERFACE
# ============================================================================

print("\n" + "="*80)
print("üé® CREATING GRADIO INTERFACE")
print("="*80)

custom_css = """
.malicious {
    color: red !important;
    font-weight: bold !important;
    font-size: 24px !important;
}
.benign {
    color: green !important;
    font-weight: bold !important;
    font-size: 24px !important;
}
"""

def gradio_predict(file):
    """Wrapper function for Gradio"""
    result = predict_file_malware(file)

    return (
        result.get("prediction", "N/A"),
        result.get("confidence", "0%"),
        result.get("threat_level", "N/A"),
        result.get("details", ""),
        result.get("rf_confidence", "0%"),
        result.get("xgb_confidence", "0%"),
        result.get("nn_confidence", "0%")
    )

with gr.Blocks(title="üõ°Ô∏è Malware Detection System", css=custom_css, theme=gr.themes.Soft()) as demo:

    gr.Markdown(
        """
        # üõ°Ô∏è AI-Powered Malware Detection System

        ### Upload a file (PDF, DOCX, DOC, or JSON) to detect if it contains malicious content

        **Powered by:** Ensemble Machine Learning (Random Forest + XGBoost + Neural Network)
        **Dataset:** CIC-Evasive-PDFMal2022 (10,025 samples)
        **Accuracy:** {:.2f}% | **Precision:** {:.2f}% | **Recall:** {:.2f}%

        ---
        """.format(accuracy * 100, precision * 100, recall * 100)
    )

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### üì§ Upload File")
            file_input = gr.File(
                label="Select File",
                file_types=[".pdf", ".docx", ".doc", ".json"],
                type="filepath"
            )

            scan_button = gr.Button("üîç Scan for Malware", variant="primary", size="lg")

            gr.Markdown(
                """
                ### ‚ÑπÔ∏è Instructions:
                1. Upload a PDF, DOCX, DOC, or JSON file
                2. Click "Scan for Malware"
                3. Wait for analysis results
                4. Review the detailed report

                **Maximum file size:** 50 MB
                """
            )

        with gr.Column(scale=2):
            gr.Markdown("### üìä Scan Results")

            with gr.Row():
                prediction_output = gr.Textbox(label="üéØ Prediction", interactive=False, lines=1)
                confidence_output = gr.Textbox(label="üìà Confidence", interactive=False, lines=1)
                threat_output = gr.Textbox(label="‚ö†Ô∏è Threat Level", interactive=False, lines=1)

            gr.Markdown("### ü§ñ Individual Model Results")

            with gr.Row():
                rf_conf = gr.Textbox(label="üå≤ Random Forest", interactive=False, lines=1)
                xgb_conf = gr.Textbox(label="‚ö° XGBoost", interactive=False, lines=1)
                nn_conf = gr.Textbox(label="üß† Neural Network", interactive=False, lines=1)

            details_output = gr.Markdown(label="üìÑ Detailed Report")

    with gr.Accordion("üìà Model Performance Metrics", open=False):
        gr.Markdown(
            f"""
            ### Overall System Performance

            | Metric | Value |
            |--------|-------|
            | **Accuracy** | {accuracy:.4f} ({accuracy*100:.2f}%) |
            | **Precision** | {precision:.4f} ({precision*100:.2f}%) |
            | **Recall** | {recall:.4f} ({recall*100:.2f}%) |
            | **F1-Score** | {f1:.4f} |
            | **ROC-AUC** | {roc_auc:.4f} |
            | **False Positive Rate** | {fpr:.4f} ({fpr*100:.2f}%) |

            ### Confusion Matrix

            | | Predicted Benign | Predicted Malicious |
            |---|-----------------|---------------------|
            | **Actual Benign** | {cm[0,0]} | {cm[0,1]} |
            | **Actual Malicious** | {cm[1,0]} | {cm[1,1]} |

            ### Individual Model Accuracy

            | Model | Accuracy |
            |-------|----------|
            | Random Forest | {model_scores['Random Forest']:.4f} ({model_scores['Random Forest']*100:.2f}%) |
            | XGBoost | {model_scores['XGBoost']:.4f} ({model_scores['XGBoost']*100:.2f}%) |
            | Neural Network | {model_scores['Neural Network']:.4f} ({model_scores['Neural Network']*100:.2f}%) |
            | **Ensemble** | **{accuracy:.4f} ({accuracy*100:.2f}%)** |
            """
        )

        gr.Image("model_evaluation.png", label="Evaluation Visualizations")

    with gr.Accordion("üìö Dataset Information", open=False):
        gr.Markdown(
            f"""
            ### CIC-Evasive-PDFMal2022 Dataset

            **Source:** Canadian Institute for Cybersecurity (CIC)
            **Total Samples:** {df.shape[0]:,}
            **Features:** {df.shape[1] - 1}
            **Classes:** Benign, Malicious

            **Class Distribution:**
            - Malicious: {np.sum(y == 1):,} samples
            - Benign: {np.sum(y == 0):,} samples

            **Top 10 Most Important Features:**

            {chr(10).join([f"{i+1}. {feature_names[idx]}" for i, idx in enumerate(top_features_idx)])}
            """
        )

    scan_button.click(
        fn=gradio_predict,
        inputs=[file_input],
        outputs=[
            prediction_output,
            confidence_output,
            threat_output,
            details_output,
            rf_conf,
            xgb_conf,
            nn_conf
        ]
    )

print("‚úÖ Gradio interface created!")

# ============================================================================
# SECTION 9: SAVE MODELS & LAUNCH
# ============================================================================

print("\n" + "="*80)
print("üíæ SAVING MODELS")
print("="*80)

# Create model directory
MODEL_SAVE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'trained_model')
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# Save all models and related data
joblib.dump(trained_models, os.path.join(MODEL_SAVE_DIR, 'trained_models.pkl'))
joblib.dump(scaler, os.path.join(MODEL_SAVE_DIR, 'scaler.pkl'))
joblib.dump(feature_names, os.path.join(MODEL_SAVE_DIR, 'feature_names.pkl'))
joblib.dump(feature_importance, os.path.join(MODEL_SAVE_DIR, 'feature_importance.pkl'))
joblib.dump(feature_stats, os.path.join(MODEL_SAVE_DIR, 'feature_stats.pkl'))

print(f"‚úÖ All models saved to: {MODEL_SAVE_DIR}")

# ============================================================================
# SECTION 10: LAUNCH THE APPLICATION
# ============================================================================

print("\n" + "="*80)
print("üöÄ LAUNCHING MALWARE DETECTION SYSTEM")
print("="*80)
print("‚úÖ Models saved successfully!")

print("\nüåê Launching Gradio interface...")
demo.launch(share=True, debug=True, show_error=True)

print("\n‚úÖ Application is now running!")
print("üéâ Upload files to detect malware!")

